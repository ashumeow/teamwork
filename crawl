#!/usr/bin/env python

import os
import json

from config.configer import Configer
from crawler.query.query import Query
from crawler.query.builder import Builder
from crawler.crawler import Crawler
from db.driver import Driver

MEMBERS_PATH = os.path.join(os.path.dirname(__file__), './raw/members.json')
MEMBERS_FILE = os.path.abspath(MEMBERS_PATH)

REPOS_PATH = os.path.join(os.path.dirname(__file__), './raw/repo.json')
REPOS_FILE = os.path.abspath(REPOS_PATH)

def main():
    configer = Configer()
    config = configer.config('teamwork')
    query_config = Configer().config('bigquery')

    crawler = Crawler(config['organization'].lower())
    crawler.add_members()
    crawler.add_repo_detail_in_file()
    
    query = Query(query_config)
    builder = Builder(config['organization'].lower())

    driver = Driver(config)
    driver.connect()

    result = query.execute(builder.build())
    # result = []

    if (result == [] or result is None):
        pass
    else:
        driver.insert('raw', result)

    with open(MEMBERS_FILE, 'r') as f:
        members = json.loads(f.read())
        f.close()

    driver.filter_rows(members)

    with open(REPOS_FILE, 'r') as f:
        repos = json.loads(f.read())
        f.close()

    if (repos == [] or repos is None):
        print "Repos insertion failed as there are None."
        pass
    else:
        driver.insert('repos', repos)


if __name__ == "__main__":
    main()
